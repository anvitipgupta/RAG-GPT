{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPZTuKGbenS97IISsOE3yHW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anvitipgupta/RAG-GPT/blob/main/RAG_based_ChatBot_File1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjBo5ImUwfVo"
      },
      "outputs": [],
      "source": [
        "!pip install --q unstructured langchain langchain-community\n",
        "!pip install --q \"unstructured[all-docs]\" ipywidgets tqdm\n",
        "!pip install ollama\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh # download ollama api\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Create a Python script to start the Ollama API server in a separate thread\n",
        "\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()"
      ],
      "metadata": {
        "id": "GjQoErCudqqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull nomic-embed-text"
      ],
      "metadata": {
        "id": "NuWWFA3gdxwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
        "from langchain_community.document_loaders import OnlinePDFLoader\n",
        "from IPython.display import display as Markdown\n",
        "from tqdm.autonotebook import tqdm as notebook_tqdm"
      ],
      "metadata": {
        "id": "OHy8uRbVktPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_path = \"/content/Accenture-Reinvention-in-the-age-of-generative-AI-Report.pdf\"\n",
        "\n",
        "# Local PDF file uploads\n",
        "if local_path:\n",
        "  loader = UnstructuredPDFLoader(file_path=local_path)\n",
        "  data = loader.load()\n",
        "else:\n",
        "  print(\"Upload a PDF file\")"
      ],
      "metadata": {
        "id": "-hLWWjhllQjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(data[0].page_content)"
      ],
      "metadata": {
        "id": "uXb5ylyImoXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. First clean up any existing ChromaDB installations\n",
        "!pip uninstall -y chromadb\n",
        "!pip uninstall -y protobuf\n",
        "\n",
        "# 2. Install specific versions known to work together\n",
        "!pip install -q protobuf==3.20.3\n",
        "!pip install -q chromadb==0.4.22  # Using a stable older version\n",
        "!pip install -q langchain-ollama\n",
        "\n",
        "# 3. Set the environment variable\n",
        "import os\n",
        "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
      ],
      "metadata": {
        "id": "P0ugkURgtZQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "KxvLQk1Lt1hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "twGC-udYtte_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split and chunk\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
        "chunks = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "EdYrJI-luKnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Now reimport with the new versions\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# 5. Try creating the vector database\n",
        "vector_db = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
        "    collection_name=\"local-rag\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "bYCxsvE6hZM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_ollama.chat_models import ChatOllama\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever"
      ],
      "metadata": {
        "id": "JOOY9C4ZhxD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3.2"
      ],
      "metadata": {
        "id": "4k8qAkoGh5ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM from Ollama\n",
        "local_model = \"llama3.2\"\n",
        "llm = ChatOllama(model=local_model)\n"
      ],
      "metadata": {
        "id": "VgrPJ18Qh1O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "    different versions of the given user question to retrieve relevant documents from\n",
        "    a vector database. By generating multiple perspectives on the user question, your\n",
        "    goal is to help the user overcome some of the limitations of the distance-based\n",
        "    similarity search. Provide these alternative questions separated by newlines.\n",
        "    Original question: {question}\"\"\",\n",
        ")"
      ],
      "metadata": {
        "id": "Y7RnpbAKh8iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = MultiQueryRetriever.from_llm(\n",
        "    vector_db.as_retriever(),\n",
        "    llm,\n",
        "    prompt=QUERY_PROMPT\n",
        ")\n",
        "\n",
        "# RAG prompt\n",
        "template = \"\"\"Answer the question based ONLY on the following context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "CegO52tfh_CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "gq9ugU-0iB1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"What are the seven components of the digital core?\")"
      ],
      "metadata": {
        "id": "08dmbM_ZiG7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete all collections in the db\n",
        "vector_db.delete_collection()\n"
      ],
      "metadata": {
        "id": "KKQz2FWNiJoL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}